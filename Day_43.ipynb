{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19b05872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/'\n",
    "                      'machine-learning-databases/wine/wine.data',\n",
    "                      header=None)\n",
    "\n",
    "df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',\n",
    "                   'Alcalinity of ash', 'Magnesium', 'Total phenols',\n",
    "                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',\n",
    "                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',\n",
    "                   'Proline']\n",
    "\n",
    "# if the Wine dataset is temporarily unavailable from the\n",
    "# UCI machine learning repository, un-comment the following line\n",
    "# of code to load the dataset from a local path:\n",
    "\n",
    "# df_wine = pd.read_csv('wine.data', header=None)\n",
    "\n",
    "# drop 1 class\n",
    "df_wine = df_wine[df_wine['Class label'] != 1]\n",
    "\n",
    "y = df_wine['Class label'].values\n",
    "X = df_wine[['Alcohol', 'OD280/OD315 of diluted wines']].values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "            train_test_split(X, y, \n",
    "                             test_size=0.2, \n",
    "                             random_state=1,\n",
    "                             stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a1f5351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = xgb.XGBClassifier(n_estimators=1000, learning_rate=0.01,\n",
    "                           max_depth=4, random_state=1,\n",
    "                           use_label_encoder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a45e213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:43:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGboost train/test accuracies 0.968/0.917\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gbm = model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = gbm.predict(X_train)\n",
    "y_test_pred = gbm.predict(X_test)\n",
    "\n",
    "gbm_train = accuracy_score(y_train, y_train_pred)\n",
    "gbm_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "\n",
    "print(f'XGboost train/test accuracies 'f'{gbm_train:.3f}/{gbm_test:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bad6a2a",
   "metadata": {},
   "source": [
    "### Here, we fit the gradient boosting classifier with 1,000 trees (rounds) and a learning rate of 0.01. Typically, a learning rate between 0.01 and 0.1 is recommended. However, remember that the learning rate is used for scaling the predictions from the individual rounds. So, intuitively, the lower the learning rate, the more estimators are required to achieve accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d590484d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
