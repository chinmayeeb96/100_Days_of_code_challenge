{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a8260b9",
   "metadata": {},
   "source": [
    "# Calculus for Machine Learning Day_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744220f8",
   "metadata": {},
   "source": [
    "## Backpropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39f5b978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training - loss value [[693.62972747]] accuracy 0.5\n",
      "Iteration 0 - loss value [[693.62972747]] accuracy 0.5\n",
      "Iteration 1 - loss value [[693.62166655]] accuracy 0.5\n",
      "Iteration 2 - loss value [[693.61534159]] accuracy 0.5\n",
      "Iteration 3 - loss value [[693.60994018]] accuracy 0.5\n",
      "Iteration 4 - loss value [[693.60515795]] accuracy 0.5\n",
      "Iteration 5 - loss value [[693.60082044]] accuracy 0.5\n",
      "Iteration 6 - loss value [[693.59683747]] accuracy 0.5\n",
      "Iteration 7 - loss value [[693.59314676]] accuracy 0.5\n",
      "Iteration 8 - loss value [[693.5896873]] accuracy 0.5\n",
      "Iteration 9 - loss value [[693.58643119]] accuracy 0.5\n",
      "Iteration 10 - loss value [[693.58333162]] accuracy 0.5\n",
      "Iteration 11 - loss value [[693.58036603]] accuracy 0.5\n",
      "Iteration 12 - loss value [[693.57752348]] accuracy 0.5\n",
      "Iteration 13 - loss value [[693.57478667]] accuracy 0.5\n",
      "Iteration 14 - loss value [[693.57214202]] accuracy 0.5\n",
      "Iteration 15 - loss value [[693.56957462]] accuracy 0.5\n",
      "Iteration 16 - loss value [[693.56707586]] accuracy 0.5\n",
      "Iteration 17 - loss value [[693.56464362]] accuracy 0.5\n",
      "Iteration 18 - loss value [[693.56227014]] accuracy 0.5\n",
      "Iteration 19 - loss value [[693.55994246]] accuracy 0.5\n",
      "Iteration 20 - loss value [[693.5576544]] accuracy 0.5\n",
      "Iteration 21 - loss value [[693.55540145]] accuracy 0.5\n",
      "Iteration 22 - loss value [[693.55317958]] accuracy 0.5\n",
      "Iteration 23 - loss value [[693.55098478]] accuracy 0.5\n",
      "Iteration 24 - loss value [[693.54881329]] accuracy 0.5\n",
      "Iteration 25 - loss value [[693.54666177]] accuracy 0.5\n",
      "Iteration 26 - loss value [[693.5445273]] accuracy 0.5\n",
      "Iteration 27 - loss value [[693.54240705]] accuracy 0.5\n",
      "Iteration 28 - loss value [[693.54029831]] accuracy 0.5\n",
      "Iteration 29 - loss value [[693.53819842]] accuracy 0.5\n",
      "Iteration 30 - loss value [[693.53610471]] accuracy 0.5\n",
      "Iteration 31 - loss value [[693.53401451]] accuracy 0.5\n",
      "Iteration 32 - loss value [[693.53192505]] accuracy 0.5\n",
      "Iteration 33 - loss value [[693.52983341]] accuracy 0.5\n",
      "Iteration 34 - loss value [[693.52773618]] accuracy 0.5\n",
      "Iteration 35 - loss value [[693.5256298]] accuracy 0.5\n",
      "Iteration 36 - loss value [[693.52351051]] accuracy 0.5\n",
      "Iteration 37 - loss value [[693.52137475]] accuracy 0.5\n",
      "Iteration 38 - loss value [[693.51921787]] accuracy 0.5\n",
      "Iteration 39 - loss value [[693.51703291]] accuracy 0.5\n",
      "Iteration 40 - loss value [[693.51480672]] accuracy 0.5\n",
      "Iteration 41 - loss value [[693.51253633]] accuracy 0.5\n",
      "Iteration 42 - loss value [[693.51021849]] accuracy 0.5\n",
      "Iteration 43 - loss value [[693.50784395]] accuracy 0.5\n",
      "Iteration 44 - loss value [[693.50540015]] accuracy 0.5\n",
      "Iteration 45 - loss value [[693.50287628]] accuracy 0.5\n",
      "Iteration 46 - loss value [[693.50025782]] accuracy 0.5\n",
      "Iteration 47 - loss value [[693.49753386]] accuracy 0.5\n",
      "Iteration 48 - loss value [[693.4946915]] accuracy 0.5\n",
      "Iteration 49 - loss value [[693.49168574]] accuracy 0.5\n",
      "Iteration 50 - loss value [[693.48849362]] accuracy 0.5\n",
      "Iteration 51 - loss value [[693.48506738]] accuracy 0.5\n",
      "Iteration 52 - loss value [[693.48136697]] accuracy 0.5\n",
      "Iteration 53 - loss value [[693.47734281]] accuracy 0.5\n",
      "Iteration 54 - loss value [[693.47290345]] accuracy 0.5\n",
      "Iteration 55 - loss value [[693.46790337]] accuracy 0.5\n",
      "Iteration 56 - loss value [[693.46218985]] accuracy 0.5\n",
      "Iteration 57 - loss value [[693.45554275]] accuracy 0.5\n",
      "Iteration 58 - loss value [[693.44730115]] accuracy 0.5\n",
      "Iteration 59 - loss value [[693.43668907]] accuracy 0.5\n",
      "Iteration 60 - loss value [[693.42344913]] accuracy 0.5\n",
      "Iteration 61 - loss value [[693.40707519]] accuracy 0.5\n",
      "Iteration 62 - loss value [[693.38932516]] accuracy 0.5\n",
      "Iteration 63 - loss value [[693.3711731]] accuracy 0.5\n",
      "Iteration 64 - loss value [[693.35194155]] accuracy 0.5\n",
      "Iteration 65 - loss value [[693.33133414]] accuracy 0.5\n",
      "Iteration 66 - loss value [[693.30857931]] accuracy 0.5\n",
      "Iteration 67 - loss value [[693.2833329]] accuracy 0.5\n",
      "Iteration 68 - loss value [[693.25547887]] accuracy 0.5\n",
      "Iteration 69 - loss value [[693.22471545]] accuracy 0.5\n",
      "Iteration 70 - loss value [[693.19076905]] accuracy 0.5\n",
      "Iteration 71 - loss value [[693.15359149]] accuracy 0.5\n",
      "Iteration 72 - loss value [[693.11307793]] accuracy 0.5\n",
      "Iteration 73 - loss value [[693.06929743]] accuracy 0.5\n",
      "Iteration 74 - loss value [[693.02279656]] accuracy 0.5\n",
      "Iteration 75 - loss value [[692.97352903]] accuracy 0.5\n",
      "Iteration 76 - loss value [[692.92114923]] accuracy 0.5\n",
      "Iteration 77 - loss value [[692.86540748]] accuracy 0.5\n",
      "Iteration 78 - loss value [[692.80581574]] accuracy 0.5\n",
      "Iteration 79 - loss value [[692.74149305]] accuracy 0.5\n",
      "Iteration 80 - loss value [[692.67261138]] accuracy 0.5\n",
      "Iteration 81 - loss value [[692.59815315]] accuracy 0.5\n",
      "Iteration 82 - loss value [[692.51755087]] accuracy 0.5\n",
      "Iteration 83 - loss value [[692.43089381]] accuracy 0.5\n",
      "Iteration 84 - loss value [[692.34064307]] accuracy 0.5\n",
      "Iteration 85 - loss value [[692.26736399]] accuracy 0.5\n",
      "Iteration 86 - loss value [[692.25726955]] accuracy 0.5\n",
      "Iteration 87 - loss value [[692.06877948]] accuracy 0.5\n",
      "Iteration 88 - loss value [[691.92184714]] accuracy 0.5\n",
      "Iteration 89 - loss value [[691.83522319]] accuracy 0.5\n",
      "Iteration 90 - loss value [[691.70951762]] accuracy 0.5\n",
      "Iteration 91 - loss value [[691.74840714]] accuracy 0.5\n",
      "Iteration 92 - loss value [[691.35431923]] accuracy 0.5\n",
      "Iteration 93 - loss value [[691.14605227]] accuracy 0.5\n",
      "Iteration 94 - loss value [[691.23258499]] accuracy 0.5\n",
      "Iteration 95 - loss value [[691.06656704]] accuracy 0.5\n",
      "Iteration 96 - loss value [[691.36733012]] accuracy 0.5\n",
      "Iteration 97 - loss value [[691.13460669]] accuracy 0.5\n",
      "Iteration 98 - loss value [[690.2979817]] accuracy 0.5\n",
      "Iteration 99 - loss value [[691.02916437]] accuracy 0.5\n",
      "Iteration 100 - loss value [[691.14411224]] accuracy 0.5\n",
      "Iteration 101 - loss value [[690.68907032]] accuracy 0.5\n",
      "Iteration 102 - loss value [[689.48488382]] accuracy 0.5\n",
      "Iteration 103 - loss value [[690.35457369]] accuracy 0.5\n",
      "Iteration 104 - loss value [[690.56241278]] accuracy 0.466\n",
      "Iteration 105 - loss value [[690.00368902]] accuracy 0.5\n",
      "Iteration 106 - loss value [[688.84378943]] accuracy 0.5\n",
      "Iteration 107 - loss value [[690.04753338]] accuracy 0.5\n",
      "Iteration 108 - loss value [[688.09993779]] accuracy 0.5\n",
      "Iteration 109 - loss value [[689.24260746]] accuracy 0.5\n",
      "Iteration 110 - loss value [[687.89519168]] accuracy 0.709\n",
      "Iteration 111 - loss value [[688.33872228]] accuracy 0.508\n",
      "Iteration 112 - loss value [[687.84147107]] accuracy 0.875\n",
      "Iteration 113 - loss value [[688.39173321]] accuracy 0.525\n",
      "Iteration 114 - loss value [[686.27309392]] accuracy 0.837\n",
      "Iteration 115 - loss value [[685.04667475]] accuracy 0.62\n",
      "Iteration 116 - loss value [[689.11881189]] accuracy 0.636\n",
      "Iteration 117 - loss value [[683.88323248]] accuracy 0.697\n",
      "Iteration 118 - loss value [[684.13138046]] accuracy 0.654\n",
      "Iteration 119 - loss value [[682.69805027]] accuracy 0.718\n",
      "Iteration 120 - loss value [[687.31445055]] accuracy 0.5\n",
      "Iteration 121 - loss value [[675.34162754]] accuracy 0.922\n",
      "Iteration 122 - loss value [[715.05866237]] accuracy 0.565\n",
      "Iteration 123 - loss value [[688.96748754]] accuracy 0.555\n",
      "Iteration 124 - loss value [[680.85279539]] accuracy 0.75\n",
      "Iteration 125 - loss value [[685.29061931]] accuracy 0.692\n",
      "Iteration 126 - loss value [[681.63277681]] accuracy 0.724\n",
      "Iteration 127 - loss value [[682.3873837]] accuracy 0.645\n",
      "Iteration 128 - loss value [[680.75980948]] accuracy 0.724\n",
      "Iteration 129 - loss value [[680.73191417]] accuracy 0.748\n",
      "Iteration 130 - loss value [[684.18768312]] accuracy 0.674\n",
      "Iteration 131 - loss value [[680.64438775]] accuracy 0.741\n",
      "Iteration 132 - loss value [[678.08350269]] accuracy 0.764\n",
      "Iteration 133 - loss value [[681.61475823]] accuracy 0.676\n",
      "Iteration 134 - loss value [[679.97537818]] accuracy 0.72\n",
      "Iteration 135 - loss value [[680.8380265]] accuracy 0.723\n",
      "Iteration 136 - loss value [[674.17258804]] accuracy 0.63\n",
      "Iteration 137 - loss value [[687.39820594]] accuracy 0.604\n",
      "Iteration 138 - loss value [[677.66270218]] accuracy 0.731\n",
      "Iteration 139 - loss value [[672.67301808]] accuracy 0.735\n",
      "Iteration 140 - loss value [[688.4429775]] accuracy 0.676\n",
      "Iteration 141 - loss value [[686.58844806]] accuracy 0.556\n",
      "Iteration 142 - loss value [[681.62783793]] accuracy 0.677\n",
      "Iteration 143 - loss value [[680.97638312]] accuracy 0.601\n",
      "Iteration 144 - loss value [[679.49567899]] accuracy 0.663\n",
      "Iteration 145 - loss value [[664.60120828]] accuracy 0.818\n",
      "Iteration 146 - loss value [[697.97739669]] accuracy 0.58\n",
      "Iteration 147 - loss value [[681.08653776]] accuracy 0.642\n",
      "Iteration 148 - loss value [[665.06165774]] accuracy 0.71\n",
      "Iteration 149 - loss value [[683.6170298]] accuracy 0.614\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    " \n",
    "# Find a small float to avoid division by zero\n",
    "epsilon = np.finfo(float).eps\n",
    " \n",
    "# Sigmoid function and its differentiation\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z.clip(-500, 500)))\n",
    "def dsigmoid(z):\n",
    "    s = sigmoid(z)\n",
    "    return 2 * s * (1-s)\n",
    " \n",
    "# ReLU function and its differentiation\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "def drelu(z):\n",
    "    return (z > 0).astype(float)\n",
    " \n",
    "# Loss function L(y, yhat) and its differentiation\n",
    "def cross_entropy(y, yhat):\n",
    "    \"\"\"Binary cross entropy function\n",
    "        L = - y log yhat - (1-y) log (1-yhat)\n",
    " \n",
    "    Args:\n",
    "        y, yhat (np.array): nx1 matrices which n are the number of data instances\n",
    "    Returns:\n",
    "        average cross entropy value of shape 1x1, averaging over the n instances\n",
    "    \"\"\"\n",
    "    return ( -(y.T @ np.log(yhat.clip(epsilon)) +\n",
    "               (1-y.T) @ np.log((1-yhat).clip(epsilon))\n",
    "              ) / y.shape[1] )\n",
    " \n",
    "def d_cross_entropy(y, yhat):\n",
    "    \"\"\" dL/dyhat \"\"\"\n",
    "    return ( - np.divide(y, yhat.clip(epsilon))\n",
    "             + np.divide(1-y, (1-yhat).clip(epsilon)) )\n",
    " \n",
    "class mlp:\n",
    "    '''Multilayer perceptron using numpy\n",
    "    '''\n",
    "    def __init__(self, layersizes, activations, derivatives, lossderiv):\n",
    "        \"\"\"remember config, then initialize array to hold NN parameters\n",
    "        without init\"\"\"\n",
    "        # hold NN config\n",
    "        self.layersizes = tuple(layersizes)\n",
    "        self.activations = tuple(activations)\n",
    "        self.derivatives = tuple(derivatives)\n",
    "        self.lossderiv = lossderiv\n",
    "        # parameters, each is a 2D numpy array\n",
    "        L = len(self.layersizes)\n",
    "        self.z = [None] * L\n",
    "        self.W = [None] * L\n",
    "        self.b = [None] * L\n",
    "        self.a = [None] * L\n",
    "        self.dz = [None] * L\n",
    "        self.dW = [None] * L\n",
    "        self.db = [None] * L\n",
    "        self.da = [None] * L\n",
    " \n",
    "    def initialize(self, seed=42):\n",
    "        \"\"\"initialize the value of weight matrices and bias vectors with small\n",
    "        random numbers.\"\"\"\n",
    "        np.random.seed(seed)\n",
    "        sigma = 0.1\n",
    "        for l, (n_in, n_out) in enumerate(zip(self.layersizes, self.layersizes[1:]), 1):\n",
    "            self.W[l] = np.random.randn(n_in, n_out) * sigma\n",
    "            self.b[l] = np.random.randn(1, n_out) * sigma\n",
    " \n",
    "    def forward(self, x):\n",
    "        \"\"\"Feed forward using existing `W` and `b`, and overwrite the result\n",
    "        variables `a` and `z`\n",
    " \n",
    "        Args:\n",
    "            x (numpy.ndarray): Input data to feed forward\n",
    "        \"\"\"\n",
    "        self.a[0] = x\n",
    "        for l, func in enumerate(self.activations, 1):\n",
    "            # z = W a + b, with `a` as output from previous layer\n",
    "            # `W` is of size rxs and `a` the size sxn with n the number of data\n",
    "            # instances, `z` the size rxn, `b` is rx1 and broadcast to each\n",
    "            # column of `z`\n",
    "            self.z[l] = (self.a[l-1] @ self.W[l]) + self.b[l]\n",
    "            # a = g(z), with `a` as output of this layer, of size rxn\n",
    "            self.a[l] = func(self.z[l])\n",
    "        return self.a[-1]\n",
    " \n",
    "    def backward(self, y, yhat):\n",
    "        \"\"\"back propagation using NN output yhat and the reference output y,\n",
    "        generates dW, dz, db, da\n",
    "        \"\"\"\n",
    "        # first `da`, at the output\n",
    "        self.da[-1] = self.lossderiv(y, yhat)\n",
    "        for l, func in reversed(list(enumerate(self.derivatives, 1))):\n",
    "            # compute the differentials at this layer\n",
    "            self.dz[l] = self.da[l] * func(self.z[l])\n",
    "            self.dW[l] = self.a[l-1].T @ self.dz[l]\n",
    "            self.db[l] = np.mean(self.dz[l], axis=0, keepdims=True)\n",
    "            self.da[l-1] = self.dz[l] @ self.W[l].T\n",
    " \n",
    "    def update(self, eta):\n",
    "        \"\"\"Updates W and b\n",
    " \n",
    "        Args:\n",
    "            eta (float): Learning rate\n",
    "        \"\"\"\n",
    "        for l in range(1, len(self.W)):\n",
    "            self.W[l] -= eta * self.dW[l]\n",
    "            self.b[l] -= eta * self.db[l]\n",
    " \n",
    "# Make data: Two circles on x-y plane as a classification problem\n",
    "X, y = make_circles(n_samples=1000, factor=0.5, noise=0.1)\n",
    "y = y.reshape(-1,1) # our model expects a 2D array of (n_sample, n_dim)\n",
    " \n",
    "# Build a model\n",
    "model = mlp(layersizes=[2, 4, 3, 1],\n",
    "            activations=[relu, relu, sigmoid],\n",
    "            derivatives=[drelu, drelu, dsigmoid],\n",
    "            lossderiv=d_cross_entropy)\n",
    "model.initialize()\n",
    "yhat = model.forward(X)\n",
    "loss = cross_entropy(y, yhat)\n",
    "score = accuracy_score(y, (yhat > 0.5))\n",
    "print(f\"Before training - loss value {loss} accuracy {score}\")\n",
    " \n",
    "# train for each epoch\n",
    "n_epochs = 150\n",
    "learning_rate = 0.005\n",
    "for n in range(n_epochs):\n",
    "    model.forward(X)\n",
    "    yhat = model.a[-1]\n",
    "    model.backward(y, yhat)\n",
    "    model.update(learning_rate)\n",
    "    loss = cross_entropy(y, yhat)\n",
    "    score = accuracy_score(y, (yhat > 0.5))\n",
    "    print(f\"Iteration {n} - loss value {loss} accuracy {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac374a0c",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8f1b408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "    return np.log(x)\n",
    "\n",
    "x = np.arange(0, 5000, 1)\n",
    "\n",
    "y = f(x)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(x, y)\n",
    "plt.tight_layout\n",
    "np.seterr(divide = 'ignore') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ae23be",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77431460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective: 0.8888888888889045\n",
      "Solution: [ 0.66666667  0.66666667 -1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def objective(w):\n",
    "    return w[0]**2 + w[1]**2\n",
    "\n",
    "def constraint_1(w):\n",
    "    return -1*w[2]-1\n",
    "\n",
    "def constraint_2(w):\n",
    "    return -1*(-3*w[0] - w[1] + w[2] - 1)\n",
    "\n",
    "def constraint_3(w):\n",
    "    return -1*(-w[0] - w[1] + w[2] - 1)\n",
    "\n",
    "def constraint_4(w):\n",
    "    return (w[0] + 2*w[1] + w[2] - 1)\n",
    "\n",
    "def constraint_5(w):\n",
    "    return (2*w[0] + w[1] + w[2] - 1)\n",
    "\n",
    "def constraint_6(w):\n",
    "    return (3*w[0] + 3*w[1] + w[2] - 1)\n",
    "\n",
    "w0 = np.array([1,1,1])\n",
    "\n",
    "bounds = ((-10,10), (-10,10), (-10,10))\n",
    "constraints = [\n",
    "    {\"type\":\"ineq\", \"fun\":constraint_1},\n",
    "    {\"type\":\"ineq\", \"fun\":constraint_2},\n",
    "    {\"type\":\"ineq\", \"fun\":constraint_3},\n",
    "    {\"type\":\"ineq\", \"fun\":constraint_4},\n",
    "    {\"type\":\"ineq\", \"fun\":constraint_5},\n",
    "    {\"type\":\"ineq\", \"fun\":constraint_6},\n",
    "]\n",
    "solution = minimize(objective, w0, method=\"SLSQP\", bounds=bounds, constraints=constraints)\n",
    "w = solution.x\n",
    "print(\"Objective:\", objective(w))\n",
    "print(\"Solution:\", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4272133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
